{"nbformat_minor": 2, "cells": [{"execution_count": 27, "cell_type": "code", "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import *", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 28, "cell_type": "code", "source": "def csvParse(s):\n    import csv\n    from StringIO import StringIO\n    sio = StringIO(s)\n    value = csv.reader(sio).next()\n    sio.close()\n    return value", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 29, "cell_type": "code", "source": "inspections = sc.textFile('wasbs:///HdiSamples/HdiSamples/FoodInspectionData/Food_Inspections1.csv')\\\n                 .map(csvParse)\n\n#select a few columns that will be useful for our predictive analysis and group the results as a dataframe,\nschema = StructType([\nStructField(\"id\", IntegerType(), False),\nStructField(\"name\", StringType(), False),\nStructField(\"results\", StringType(), False),\nStructField(\"violations\", StringType(), True)])\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 30, "cell_type": "code", "source": "#DataFrame\ndataFrame = sqlContext.createDataFrame(inspections.map(lambda l: (int(l[0]), l[1], l[12], l[13])) , schema)\ndataFrame.registerTempTable('CountResults')\ndataFrame.persist()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "DataFrame[id: int, name: string, results: string, violations: string]"}], "metadata": {"collapsed": false}}, {"execution_count": 31, "cell_type": "code", "source": "def labelForResults(s):\n    if s == 'Fail':\n        return 0.0\n    elif s == 'Pass w/ Conditions' or s == 'Pass':\n        return 1.0\n    else:\n        return -1.0\nlabel = UserDefinedFunction(labelForResults, DoubleType())\nlabeledData = df.select(label(df.results).alias('label'), df.violations).where('label >= 0')", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 32, "cell_type": "code", "source": "tokenizer = Tokenizer(inputCol=\"violations\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\nlr = LogisticRegression(maxIter=10, regParam=0.01)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n\nmodel = pipeline.fit(labeledData)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 33, "cell_type": "code", "source": "testData = sc.textFile('wasbs:///HdiSamples/HdiSamples/FoodInspectionData/Food_Inspections2.csv')\\\n          .map(csvParse) \\\n          .map(lambda l: (int(l[0]), l[1], l[12], l[13]))\ntestDf = sqlContext.createDataFrame(testData, schema).where(\"results = 'Fail' OR results = 'Pass' OR results = 'Pass w/ Conditions'\")\npredictionsDf = model.transform(testDf)\npredictionsDf.registerTempTable('Predictions')\npredictionsDf.columns", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['id', 'name', 'results', 'violations', 'words', 'features', 'rawPrediction', 'probability', 'prediction']"}], "metadata": {"collapsed": false}}, {"execution_count": 26, "cell_type": "code", "source": "numSuccesses = predictionsDf.where(\"\"\"(prediction = 0 AND results = 'Fail') OR\n                                       (prediction = 1 AND (results = 'Pass' OR\n                                                            results = 'Pass w/ Conditions'))\"\"\").count()\nnumInspections = predictionsDf.count()\n\nprint \"There were\", numInspections, \"inspections and there were\", numSuccesses, \"successful predictions\"\nprint \"This is a\", str((float(numSuccesses) / float(numInspections)) * 100) + \"%\", \"success rate\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "There were 9315 inspections and there were 8087 successful predictions\nThis is a 86.8169618894% success rate"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}